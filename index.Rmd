---
title:
author: "cjlortie"
date: "2019"
output:
  html_document:
    theme: spacelab
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
---
<br>  

### A scrape of Mueller report.  

![](./mr.jpeg)   

### Purpose
To quantitatively examine the report.  

### The report
```{r, warning=FALSE, message=FALSE}
#library(pdftools)
#report into useable object
#report <- file.path("https://assets.documentcloud.org/documents/5955379/Redacted-Mueller-Report.pdf")
#report_text <- pdf_text(report)

#word count
#wc<-stri_count_words(report_text, locale = NULL)
#wc

#df conversion from Ohara
#https://github.com/oharac/text_workshop
#report_df <- data.frame(text = report_text) 

#report_df <- data.frame(text = report_text) %>%
  #mutate(page = 1:n()) %>%
  #mutate(text_sep = str_split(text, '\\n')) %>% # split by line, text_set=lists of lines 
  #unnest(text_sep) # separate lists into rows

#report_df <- data.frame(text = report_text) %>%
  #mutate(page = 1:n()) %>%
  #mutate(text_sep = str_split(text, '\\n')) %>%
  #unnest(text_sep) %>%
  #group_by(page) %>%
  #mutate(line = 1:n()) %>% # add line #s by page
  #ungroup()

#write_csv(report_df, "data/report_df.csv")
#too large for github so scraped, wrote, and pushed to figshare  
#report <- read_csv("https://ndownloader.figshare.com/files/14930555")
#saveRDS(report, file = "data/report.rds")

```

### Tidytext mining  
```{r, tidytext, warning=FALSE, message=FALSE}
#https://www.tidytextmining.com/tidytext.html
#key libraries####
library(tidyverse)
library(tidytext)
library(wordcloud)
library(stringr)

#load and tidy up text####
report <- readRDS("data/report.rds")
text_df <- tibble(line = 1:19195, text = report$text_sep)

text_df <-text_df %>%
  unnest_tokens(word, text)

data(stop_words) #common works like 'the', 'that', 'and', 'with'
tidy_df <- text_df %>%
  anti_join(stop_words) #remove stop words

#some of the words in character vector are numeric so tidy up
tidy_numbers <- text_df %>%
  filter(!is.na(as.numeric(word)))

tidy_df <- tidy_df %>%
  anti_join(tidy_numbers)

#word count
word_count <- tidy_df %>%
  count(word, sort = TRUE) %>%
  mutate(proportion = n / sum(n))
word_count
#write_csv(word_count, "data/mueller_report_words.csv")

#viz of word frequencies####
na.exclude(tidy_df) %>%
  count(word, sort = TRUE) %>%
  filter(n > 200) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  theme(text = element_text(size=10))

wordcloud(word_count$word, word_count$n, max.words = 100, random.order = FALSE, scale = c(3, .7))

#sentiments####
sentiments
#The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. 
#The bing lexicon categorizes words in a binary fashion into positive and negative categories. 
#The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. 

bing_sent <- get_sentiments("bing")
tidy_sent_bing <- inner_join(tidy_df, bing_sent, by = "word") #retain only rows in both sets.
sentiment_summary <- tidy_sent_bing %>%
  count(word, index = line %/% 6838, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(net_sentiment = positive - negative)

ggplot(sentiment_summary, aes(net_sentiment)) +
         geom_histogram(binwidth = 0.5) +
  xlim(-20, 20) +
  labs(x = "net sentiment (positive-negative)", y = "word frequency") +annotate("text", x=10, y=400, label= "net sentiment = -462") +
  geom_vline(xintercept = 0, linetype = 2, color = "red", size = 1)

ggplot(sentiment_summary, aes(net_sentiment)) +
         geom_histogram(binwidth = 1) +
  xlim(-100, 100) +
  labs(x = "net sentiment (positive-negative)", y = "word frequency") +annotate("text", x=50, y=400, label= "net sentiment = -462") +
  geom_vline(xintercept = 0, linetype = 2, color = "red", size = 1)

sum(sentiment_summary$net_sentiment)
```

### Smaller pdf test
```{r, pdf test, warning=FALSE, message=FALSE}
library(tidyverse)
library(pdftools)
library(tidytext)
library(wordcloud)

#step 1. get pdf into text
text <- pdf_text("report_smaller.pdf")

#step 2. convert to tibble
text_df <- tibble(text = text) %>%
  unnest_tokens(word, text) %>%
  mutate(linenumber = row_number())

#step 3. remove stopwords
data("stop_words")
tidy_df <- text_df %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) 

#step 4. tidy up for words that are not words
tidy_numbers <- tidy_df %>%
  filter(!is.na(as.numeric(word)))

tidy_df <- tidy_df %>%
  anti_join(tidy_numbers)

#regex needed now to really clean it up
#shortcut - filter tidy_df for all n>1?
#BUT if doing sentiment analysis - all words that do not have sentiment assigned will be filtered out by anti_join

#write_csv(tidy_df, "data/tidy.csv")

#plot of frequency
tidy_df %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  theme(text = element_text(size=6))

#word cloud
wordcloud(tidy_df$word, tidy_df$n, max.words = 100, random.order = FALSE, scale = c(3, .7))

#sentiment
bing_sent <- get_sentiments("bing")
tidy_sent_bing <- inner_join(tidy_df, bing_sent, by = "word")
sentiment_summary <- tidy_sent_bing %>%
  count(word, index = 1:length(n), sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(net_sentiment = positive - negative)

ggplot(sentiment_summary, aes(net_sentiment)) +
         geom_histogram() +
  xlim(-20, 20) +
  labs(x = "net sentiment (positive-negative)", y = "word frequency") +annotate("text", x=10, y=400, label= "net sentiment = -462") +
  geom_vline(xintercept = 0, linetype = 2, color = "red", size = 1)

#odd code in some respects
tidy_sent <- tidy_sent_bing %>%
  group_by(sentiment) %>%
  summarise(total = sum(n)) 
tidy_sent


```

### Tweets  
```{r, tweets, warning=FALSE, message=FALSE}
#install.packages("twitteR")
#library(twitteR)
#tweets <- searchTwitter("#MuellerReport",n=1000,lang="en")
#tweets <- twListToDF(tweets)
#write_csv(tweets_df,"data/tweets.csv")
tweets <- read_csv("data/tweets.csv")
tweets_df <- tweets %>% 
  select(id, text) %>% 
  unnest_tokens(word,text)

#step 3. remove stopwords
data("stop_words")
tweets_df <- tweets_df %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) 

#step 4. tidy up for words that are not words
tidy_numbers <- tweets_df %>%
  filter(!is.na(as.numeric(word)))

tidy_tweets <- tweets_df %>%
  anti_join(tidy_numbers)

#plot
tidy_tweets %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  theme(text = element_text(size=6))

#word cloud
wordcloud(tidy_tweets$word, tidy_tweets$n, max.words = 100, random.order = FALSE, scale = c(3, .7))
```
