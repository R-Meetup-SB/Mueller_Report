---
title:
author: "cjlortie"
date: "2019"
output:
  html_document:
    theme: spacelab
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
---
<br>  

### A scrape of Mueller report.  

![](./mr.jpeg)   

### Purpose
To quantitatively examine the report.  

### Report
```{r, warning=FALSE, message=FALSE}
#library(pdftools)

#report into useable object
#report <- file.path("https://assets.documentcloud.org/documents/5955379/Redacted-Mueller-Report.pdf")
#report_text <- pdf_text(report)

#word count
#wc<-stri_count_words(report_text, locale = NULL)
#wc

#df conversion from Ohara
#https://github.com/oharac/text_workshop
#report_df <- data.frame(text = report_text) 

#report_df <- data.frame(text = report_text) %>%
  #mutate(page = 1:n()) %>%
  #mutate(text_sep = str_split(text, '\\n')) %>% # split by line, text_set=lists of lines 
  #unnest(text_sep) # separate lists into rows

#report_df <- data.frame(text = report_text) %>%
  #mutate(page = 1:n()) %>%
  #mutate(text_sep = str_split(text, '\\n')) %>%
  #unnest(text_sep) %>%
  #group_by(page) %>%
  #mutate(line = 1:n()) %>% # add line #s by page
  #ungroup()

#write_csv(report_df, "data/report_df.csv")

#tidytext book example
#https://www.tidytextmining.com/tidytext.html
#library(tidytext)
#text_df <- tibble(line = 1:212, text = report_text)
#text_df %>%
  #unnest_tokens(word, text)

#too large for github so scraped, wrote, and pushed to figshare  
#report <- read_csv("https://ndownloader.figshare.com/files/14930555")
#saveRDS(report, file = "data/report.rds")

```


### Conventional text exploration  
```{r, text EDA, warning=FALSE, message=FALSE, echo = FALSE, eval = FALSE}
#old skool approach
library(wordcloud)
library(tm)
library(stringi)
library(stringr)

#read messy data
report <- readRDS("data/report.rds")

#create a corpus for tm
corpus <- Corpus(VectorSource(report))

#clean a wee bit
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
#corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, removeWords, c("the", "that", "and"))


#word cloud
wordcloud(corpus, max.words = 100, random.order = FALSE, scale = c(3, .7))

```

### Tidytext mining  
```{r, tidytext, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
report <- readRDS("data/report.rds")
text_df <- tibble(line = 1:19195, text = report$text_sep)

text_df <-text_df %>%
  unnest_tokens(word, text)

data(stop_words)

tidy_df <- text_df %>%
  anti_join(stop_words)

word_count <- tidy_df %>%
  count(word, sort = TRUE) 
word_count
#write_csv(word_count, "data/mueller_report_words.csv")

tidy_df %>%
  count(word, sort = TRUE) %>%
  filter(n > 200) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  theme(text = element_text(size=10))

```

